{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pymongo\n",
    "from pymongo import TEXT\n",
    "import dns \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions used to scrap and extract news head-lines fromm bbc's website\n",
    "\n",
    "def get_http_response():\n",
    "    #get and return html response \n",
    "    bbc_news_url = \"https://www.bbc.co.uk/news\"\n",
    "    response = requests.get(bbc_news_url)\n",
    "    return response\n",
    "\n",
    "\n",
    "def extract_articles(article_div):\n",
    "    #extract article details(headline, url, summary, date)  \n",
    "    head_line=[]\n",
    "    article_url=[]\n",
    "    article_summary=[]\n",
    "    article_date_time=[]\n",
    "\n",
    "    for article in article_div:\n",
    "        \n",
    "        try:\n",
    "            title = article.find('h3').text\n",
    "            #print(title)\n",
    "            head_line.append(title)\n",
    "        except:\n",
    "            print(\"exception while extracting head line\")\n",
    "    \n",
    "        try:\n",
    "            href = article.find('a', href=True)\n",
    "            url = \"https://www.bbc.com\"+href['href']\n",
    "            #print(url)\n",
    "            article_url.append(url)\n",
    "        except:\n",
    "            print(\"exception while extracting url\")\n",
    "        \n",
    "        try:\n",
    "            summary = article.find('p').text\n",
    "            #print(summary)\n",
    "            article_summary.append(summary)\n",
    "        except:\n",
    "            print(\"exception while extracting summary\")\n",
    "        \n",
    "        try:\n",
    "            date_time = article_div[0].find('time')['datetime']\n",
    "            #print(date_time)\n",
    "            article_date_time.append(date_time)\n",
    "        except:\n",
    "            print(\"exception while extracting date time\")\n",
    "        #print(\"\\n\")    \n",
    "    return head_line, article_url, article_summary, article_date_time\n",
    "\n",
    "\n",
    "def get_article(article_url):\n",
    "    # get http response from article url \n",
    "    # input: array of url\n",
    "    # return: array of html responses\n",
    "    article_soup=[]\n",
    "    for url in article_url:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            article_soup.append(soup(response.content, 'lxml'))\n",
    "        except:\n",
    "            print(\"error fetching article response\")\n",
    "        \n",
    "    return(article_soup)\n",
    "\n",
    "def extract_article_text(url):\n",
    "    #extract article text \n",
    "    #input:array of url\n",
    "    #return array of strings\n",
    "    \n",
    "    #get list of soup\n",
    "    article_soup = get_article(url)\n",
    "    #get list of article text blocks\n",
    "    text_blocks = article_soup[0].find_all(attrs={\"data-component\": \"text-block\"})\n",
    "    #append all text blocks in one text and append it in article text list\n",
    "    article_text=[]\n",
    "    text=\"\"\n",
    "    for article in article_soup:\n",
    "        try:\n",
    "            text_blocks = article.find_all(attrs={\"data-component\": \"text-block\"})\n",
    "        except:\n",
    "            print(\"error finding blocks\")\n",
    "        for block in text_blocks:\n",
    "            try:\n",
    "                text += block.find('p').text+\"\\n\"\n",
    "            except:\n",
    "                print(\"error extracting article text\")\n",
    "        \n",
    "        article_text.append(text)\n",
    "        text = \"\"\n",
    "        text_blocks=[]\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response retriev success\n",
      "\n",
      "10 article promos was extracted attemping to extract articles details\n",
      "\n",
      "10 article text was extracted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = get_http_response()\n",
    "if response.status_code==200:\n",
    "    print(\"response retriev success\\n\")\n",
    "    bbc_soup = soup(response.content, 'lxml')\n",
    "    article_div = bbc_soup.find_all( attrs={\"class\": \"gs-c-promo-body gel-1/2@xs gel-1/1@m\"})\n",
    "    print(\"{} article promos was extracted attemping to extract articles details\\n\".format(len(article_div)))\n",
    "    head_line, article_url, article_summary, article_date_time = extract_articles(article_div);\n",
    "    article_text = extract_article_text(article_url)\n",
    "    print(\"{} article text was extracted \".format(len(article_text)))\n",
    "else:\n",
    "    print(\"Connection not possible code:{}\".format(response.response.status_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head_line</th>\n",
       "      <th>url</th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nashville blast 'probably suicide bombing'</td>\n",
       "      <td>https://www.bbc.com/news/world-us-canada-55456899</td>\n",
       "      <td>Investigators conduct DNA tests after human re...</td>\n",
       "      <td>2020-12-27T02:08:27.000Z</td>\n",
       "      <td>The camper van blast in Nashville on Christmas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    head_line  \\\n",
       "0  Nashville blast 'probably suicide bombing'   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.bbc.com/news/world-us-canada-55456899   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Investigators conduct DNA tests after human re...   \n",
       "\n",
       "                       date                                          full_text  \n",
       "0  2020-12-27T02:08:27.000Z  The camper van blast in Nashville on Christmas...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataframe the article details\n",
    "data = {'head_line': head_line, 'url': article_url, 'summary':article_summary, 'date': article_date_time, \n",
    "       'full_text' : article_text}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Database(MongoClient(host=['mflix-shard-00-02.pzqmc.mongodb.net:27017', 'mflix-shard-00-00.pzqmc.mongodb.net:27017', 'mflix-shard-00-01.pzqmc.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, retrywrites=True, w='majority', authsource='admin', replicaset='atlas-140cqu-shard-0', ssl=True), 'test')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = pymongo.MongoClient(\"mongodb+srv://m220student:m220password@mflix.pzqmc.mongodb.net/admin?retryWrites=true&w=majority\")\n",
    "client.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article inserted\n",
      "article inserted\n",
      "article inserted\n",
      "article inserted\n",
      "article inserted\n",
      "article inserted\n",
      "article inserted\n",
      "article inserted\n",
      "article inserted\n",
      "article inserted\n",
      "df inserted\n"
     ]
    }
   ],
   "source": [
    "db = client.news\n",
    "for row in df.iterrows():\n",
    "    \n",
    "    article={\n",
    "        'head_line':row[1]['head_line'], 'url':row[1]['url'], 'summary':row[1]['summary'],\n",
    "         'date':row[1]['date'], 'article_text':row[1]['full_text']\n",
    "                                                      }\n",
    "    db.bbc_articles.insert_one(article)\n",
    "    print(\"article inserted\")\n",
    "    \n",
    "print(\"df inserted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'article_text_text'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create text index to search articles by keyword\n",
    "db.bbc_articles.create_index([('article_text', TEXT)], default_language='english')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_cursor = db.bbc_articles.find({\"$text\": {\"$search\": \"real\"}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-a60da1cebb37>:1: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  db.bbc_articles.find({\"$text\": {\"$search\": \"real\"}}).count()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.bbc_articles.find({\"$text\": {\"$search\": \"real\"}}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRA 'wanted to exclude Sinn Féin from peace talks'\n",
      "Irish government papers from 1990 suggest some IRA bosses did not share the party's socialist views.\n"
     ]
    }
   ],
   "source": [
    "for article in find_cursor:\n",
    "    print(article['head_line']+\"\\n\"+article['summary'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
